\tool{Conclusioni e direzioni di sviluppo}

\textit{
Questo capitolo riassume le motivazioni alla base di questo lavoro e sintetizza le soluzioni che sono state proposte. Il paragrafo~\ref{sec_it:discussione} discute delle analogie con sistemi similari, mentre il paragrafo~\ref{sec_it:sviluppi_futuri} traccia le linee guida per lo sviluppo futuro del progetto.
}

\sezione{Riepilogo e contributo}
La localizzazione di contenuti è un problema di carattere generale nelle reti p2p. I più diffusi sistemi p2p di distribuzione di contenuti delegano il servizio di ricerca a un sistema basato sul modello client-server nel quale il server ha una visione globale dei contenuti disponibili. Ciò richiede in generale un considerevole investimento in termini di infrastrutture al fine di ottenere elevati livelli di scalabilità. Un tale approccio è inoltre incline al sorgere di problematiche legate alla sicurezza ed alla riservatezza dei dati che sono concentrati in un unico punto.

\subsezione{Contributo}
Questo lavoro ha avuto come scopo l'indagine dei modelli alla base di un sistema informativo completamente decentralizzato che offra a utenti accademici un servizio di ricerca e di raccomandazioni di articoli scientifici. L'approccio utilizzato consiste nell'organizzare gli agenti partecipanti in una topologia di interconnessione che possa essere sfruttata per facilitare la localizzazione di contenuti messi a disposizione da ciascun agente.

Facendo uso di un protocollo epidemico a bassa intrusione la topologia di interconnessione tra gli agenti viene proattivamente e dinamicamente adattata alle loro esigenze informative. Tale topologia individua un social network che  aggrega utenti con simili esigenze informative, le quali sono automaticamente stimate sulla base dell'analisi delle loro abitudini di lettura. Ciascun utente colleziona localmente gli articoli scientifici letti, dai quali vengono automaticamente estratte una serie di informazioni strutturate, quali il titolo, l'abstract, l'intero contenuto testuale e le citazioni, che vengono localmente indicizzate. Le esigenze informative dell'utente possono essere stimate analizzando il contenuto di questo database. Date le caratteristiche essenzialmente testuali degli articoli scientifici, la semantica associata può essere modellata facendo ricorso al modello a spazio vettoriale del testo che si basa sull'analisi statistica delle ricorrenze lessicali nel testo. La similarità semantica tra due documenti testuali, modellata su base lessicale, è riconducibile al computo di distanze tra vettori di uno spazio vettoriale. Le esigenze informative dell'utente vengono stimate computando semplicemente la media della semantica associata a ciascun documento. Tali esigenze informative, modellate da un vettore in uno spazio vettoriale ad alta dimensionalità, vengono approssimate con un vettore a dimensione ridotta ottenuto selezionando le $k$ componenti di maggior peso. Ciò è necessario affinché il protocollo epidemico adottato per la gestione della topologia abbia una bassa intrusione. La ricerca e la raccomandazione agli utenti di contenuti ritenuti di interesse è basata sfruttando il social network che, per come è costruito, soddisfa localmente le esigenze informative degli utenti, dipendentemente dalla qualità della stima effettuata.

\sezione{\label{sec_it:discussione}Discussione e critica}
L'approccio alla soluzione adottata è ispirata al servizio distribuito di ricerca e raccomandazioni adottato dal sistema di file sharing Tribler~\cite{PGW+08}, che aggrega utenti con esigenze informative simili, stimate sulla base dell'analisi della storia dei file scaricati. Il nostro sistema, che ha come oggetto informativo documenti essenzialmente testuali, differisce da questo per le modalità con le quali il sistema stima le esigenze informative dell'utente.

Mendeley\footnote{\url{https://www.mendeley.com}} è un sistema di raccomandazioni per articoli scientifici, ancora in fase beta di sviluppo, con caratteristiche molto simili, per gli scopi che si propone, al nostro progetto~\cite{HR08}. Differentemente dalla nostra soluzione, che è completamente decentralizzata, Mendeley colleziona globalmente le preferenze di tutti gli utenti facendo riferimento al classico modello di computazione client-server.

%----------------------------------------------------------------------------------------------------%
%                                                                                                    %
%----------------------------------------------------------------------------------------------------%
\sezione{\label{sec_it:sviluppi_futuri}Direzioni di sviluppo}
In questo lavoro abbiamo considerato soluzioni che si sono limitate a tenere in considerazione gli aspetti fondanti di un sistema informativo completamente centralizzato per la ricerca e la raccomandazione di articoli scientifici.

Le direzioni di sviluppo dovrebbero perseguire lo stato dell'arte nella realizzazione di ciascun componente del sistema. In particolare, per ciò che riguarda l'estrazione di informazioni strutturate da documenti orientati alla presentazione, quali i documenti in formato \textsc{pdf}, lo stato dell'arte prevede l'utilizzo di sistemi di apprendimento automatico. Un approccio analogo dovrebbe essere adottato anche dal sottosistema di apprendimento delle necessità informative dell'utente, che dovrebbe basarsi sull'analisi di una serie di misure oggettive derivanti sia dall'interazione dell'utente con il sistema (e.g.\ il tempo impiegato nella lettura dei singoli articoli, le parti sulle quali l'utente si è maggiormente soffermato, l'analisi delle query sottoposte al sistema, etc.) e sia dall'analisi degli articoli letti. Per quanto concerne quest'ultimo aspetto, dovrebbe essere preso in considerazione un modello più accurato della semantica del testo, quale l'analisi della semantica latente (\textit{Latent Semantic Indexing}) che ovvia alle limitazioni del modello lessicale della semantica adottato in questo progetto.

Poiché la topologia della rete di interconnessione dipende dal profilo dell'utente e della metrica impiegati dal protocollo epidemico, questi dovrebbero essere opportunamente valutati al fine di ottenere topologie che supportino la scalabilità e la località dei contenuti, e.g.\ qualità del clustering.

\subsezione{Visione}
Il modello di computazione p2p permette di ridurre i costi associati alla collaborazione tra gruppi di utenti. Questo potenziale potrebbe essere sfruttato in ambito accademico da una piattaforma basata su un social network che faciliti la collaborazione tra gruppi di ricerca con simili interessi attraverso la condivisione di idee ed esperienze.
